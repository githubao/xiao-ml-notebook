# ml algorithms

之前比较长的一段时间，一直在接触一些机器学习的名词和算法。看了很多资料，写了很多代码，因为都是比对着别人的代码在写，所有很少会去想为什么是这样的。

说实在话，对一些算法的理解只是基于表层，甚至表层都不算。所以现在想要再重新研究，整理思路。当然，能够坚持多久，理解到什么样的程序，就自求多福了...

## ANN

人工神经网络，是神经网络最初的形式。说到底就是矩阵计算吧。比如有500个样本，特征是100维，那么目标往往是: [500,100] * [100,100] * [100,100], 然后softmax得到[500,1]的向量，用于分类问题。

当然其中还有一些softmax和dropout和batch等等的技巧。

## CNN

原始的mnist集合的图片是28*28的黑白图片，所以把输入做成[*,28,28,1]的形状，最后一个位置的1表示是黑白图片，如果是彩色(rgb)图片，那么这个数字是3。

使用[5,5,1,32]的卷积核，5*5的小框框按照步长[1,1,1,1]并且‘same’的方式一步一步地向前移动，输入是1层(就是上面输入的最后一个数据，表示色彩的1)，输出是32层(这个可以自己决定大小)，输出是[*,28,28,32]的形状，[1,2,2,1]的池化之后变成[*,14,14,32]的形状。

再来一步，使用[5,5,32,64]的卷积核一步一步地向前移动，经过池化之后变成[*,7,7,64]的形状。

第三步骤是全连接层，变成[*,1024]的形状。

再来一次全连接变成[*,10]，然后对10个数字softmax之后然后argmax预测。

CNN相比于传统的神经网络的优点是，它关注数据的局部特征，可以更好的表征数据，而且没有必要做那么多的全连接层(全连接意味着巨量的需要预估的参数)。

[参考链接](https://zhuanlan.zhihu.com/p/26139876)

## RNN

适用于序列预测的问题，从上一步到下一步，中间做什么样子函数变换的才合适。

由于梯度消失和梯度爆炸的问题，好像都用lstm了，很少见到rnn了。

## LSTM

还是mnist数据集，原来的数据是[*,28,28]的数据结构，现在把数据看做28(表示行的28，是前面三维向量的第二维)个[*,28]的序列的形式。

第二部，构建lstm神经元，隐藏层是[128]，然后把神经元和上面构建的x向量应用于rnn的计算。

然后对于输出的数据做一个[128,10]的全连接计算。

[参考链接](http://www.jianshu.com/p/cc088fcb66e5)

## k近邻算法

## 朴素贝叶斯分类

## 线性回归

## 逻辑回归

## svm分类

## 决策树

## 神经网络

## k-means聚类